{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "d:\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "d:\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "d:\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tsfresh import  select_features\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from datetime import datetime,timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,BorderlineSMOTE,ADASYN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.constants import Constants\n",
    "from utils.tracker import init_logging\n",
    "from classifier.phcp import phcp\n",
    "from sklearn import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create label and combine the separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_days(year, month, day):\n",
    "    get_day = date(year, month, day)\n",
    "    return int(get_day.strftime('%j'))  # %j represents the jth day of the year \n",
    "def get_label_shift(year,days,gap_hour,size_hour,shift_hour):\n",
    "    if(year%4==0):\n",
    "        day=366\n",
    "    else:\n",
    "        day=365\n",
    "\n",
    "    shift_windows=int(shift_hour/gap_hour)\n",
    "    windows=int((day*24-size_hour)/gap_hour)\n",
    "    windows=windows+shift_windows\n",
    "\n",
    "    df=pd.read_csv(\"new_data.csv\",index_col =0)\n",
    "    df['A'] = pd.to_datetime(df['A']) # date转为时间格式\n",
    "    df_data=df.values\n",
    "    data_year=[x for x in df_data if x[0].year==year or x[0].year==year+1]\n",
    "    label=np.zeros(windows).astype(np.int32)\n",
    "\n",
    "    begin_time=str(year)+\"-01-01-00\"\n",
    "    label_t=[]\n",
    "    begin_time=datetime.strptime(begin_time, '%Y-%m-%d-%H')\n",
    "    label_t.append(begin_time+timedelta(hours=shift_hour))\n",
    "    #label_t.append(begin_time)\n",
    "    for i in range(0,windows-1):\n",
    "        label_t.append(label_t[-1]+timedelta(hours=gap_hour))\n",
    "    label_t=np.array(label_t)\n",
    "\n",
    "\n",
    "    label_len=len(label)\n",
    "    for i in data_year:\n",
    "        dd = i[0]\n",
    "        if(dd.year==year):\n",
    "            day=get_days(dd.year, dd.month, dd.day)\n",
    "            offset=(day-1)*12+int(dd.hour/gap_hour)+1-1-shift_windows #1st begin with 0, int(dd.hour/gap_hour)+1-1 is the hour offset, +1:the remaining time, -1:begin with 0\n",
    "        elif(dd.year==year+1):\n",
    "            offset=-1\n",
    "            if(day==1 and dd.hour<shift_hour):\n",
    "                offset=(day-1)*12+int(dd.hour/gap_hour)+1-1\n",
    "                offset=label_len-shift_windows+offset\n",
    "        scale=i[1]\n",
    "        if offset<label_len and offset>=0:\n",
    "            if(scale==\"S\" and label[offset]<1):\n",
    "                \n",
    "                label[offset]=1\n",
    "            elif(scale==\"M\" ):\n",
    "                label[offset]=2\n",
    "            elif(scale==\"L\"):\n",
    "                label[offset]=3\n",
    "\n",
    "    #relabel\n",
    "    size_windows=int(size_hour/gap_hour)\n",
    "    for i in range(0,len(label)-size_windows):\n",
    "        label[i]=max(label[i:i+size_windows])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    temp=np.empty(shape=(0,0))\n",
    "    temp_t=np.empty(shape=(0,0))\n",
    "    \n",
    "    for day in days:\n",
    "        begin=int((day[0]-1)*(24/gap_hour))\n",
    "        length=int(((day[1]-day[0]+1)*24-size_hour)/gap_hour)\n",
    "        temp=np.append(temp,label[begin:begin+length])\n",
    "        temp_t=np.append(temp_t,label_t[begin:begin+length])\n",
    "    total = np.array([temp,temp_t]).T\n",
    "        \n",
    "    # result=pd.DataFrame(temp,columns=[\"label\"]) \n",
    "    result=pd.DataFrame(total,columns=[\"label\",\"time\"]) \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:198: RuntimeWarning: The columns ['HHZ.D__query_similarity_count__query_None__threshold_0.0'\n",
      " 'HHN.D__query_similarity_count__query_None__threshold_0.0'\n",
      " 'HHE.D__query_similarity_count__query_None__threshold_0.0'] did not have any finite values. Filling with zeros.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gap=4\n",
    "    size=12\n",
    "    sampling_rate=0.01\n",
    "    prefix=\"./hf_result_save/\"+str(gap)+\"_\"+str(size)+\"_\"+str(sampling_rate)+\"_\"\n",
    "\n",
    "    shift=6\n",
    "    X=pd.DataFrame()\n",
    "    y=pd.DataFrame() \n",
    "    suffix=\"_WEST.csv\"\n",
    "    days_years=[[2013,[[141,365]]],[2014,[[1,365]]],[2015,[[1,171],[182,283]]],[2016,[[19,366]]],[2017,[[1,15],[41,151]]]]\n",
    "    for i in days_years:\n",
    "        filename=prefix+str(i[0])+suffix \n",
    "        x_temp=pd.read_csv(filename,index_col=0)\n",
    "        X=pd.concat([X,x_temp]) \n",
    "        y_temp=get_label_shift(i[0],i[1],gap,size,shift)\n",
    "        y=pd.concat([y,y_temp]) \n",
    "\n",
    "    prefix=\"./hf_data_save/\"+str(gap)+\"_\"+str(size)+\"_\"+str(sampling_rate)+\"_\"\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    impute(X)\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)\n",
    "    X_train = select_features(X_train, y_train['label'], ml_task='classification', n_jobs=4)  # or boruta\n",
    "    X = X[X_train.columns]\n",
    "    X.to_csv(prefix+\"X.csv\",index=True)\n",
    "    \n",
    "    y.to_csv(prefix+\"y.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=0\n",
    "def nor_diff( x, y):\n",
    "    global scale\n",
    "    result = np.sum(np.square(abs(x - y) / scale))\n",
    "    return result\n",
    "\n",
    "def additional_attri(a,b):\n",
    "    global scale\n",
    "    X=a.values\n",
    "    y=b.values\n",
    "    scale=4 * np.nanstd(X, axis=0)\n",
    "\n",
    "    dis_metric = np.zeros((X.shape[0], X.shape[0]))\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        inx = [i] * X.shape[0]\n",
    "        for j, p in enumerate(inx):\n",
    "            dis_metric[p, j] = nor_diff(X[p], X[j])\n",
    "\n",
    "    for i, j in enumerate(range(X.shape[0])):\n",
    "        dis_metric[i, j] = 1000000\n",
    "    num_neighbor = 5\n",
    "\n",
    "    top = np.argpartition(dis_metric, num_neighbor, axis=1)[:, :num_neighbor]\n",
    "    #dis_neighbor = dis_metric[np.arange(dis_metric.shape[0])[:, None], top]  # compute neighbors for every sample\n",
    "\n",
    "\n",
    "    count_neighbor = np.zeros((X.shape[0], num_neighbor+2))  # count the types of neighbors and decide type\n",
    "    for i in tqdm(range(count_neighbor.shape[0])):\n",
    "        count_neighbor[i, :num_neighbor] = y[top[i, :]]\n",
    "        #count how many same labels with y[i]\n",
    "        count=len(np.where(count_neighbor[i, :num_neighbor]==y[i])[0])\n",
    "        count_neighbor[i, num_neighbor]=5-count\n",
    "        #count_neighbor[i, num_neighbor] = abs(5 * y[i] -np.sum(count_neighbor[i, :num_neighbor]))\n",
    "        if count_neighbor[i, num_neighbor] == 5:\n",
    "            count_neighbor[i, num_neighbor+1] = 0.1\n",
    "        elif count_neighbor[i, num_neighbor] == 4:\n",
    "            count_neighbor[i, num_neighbor+1] = 0.25\n",
    "        elif count_neighbor[i, num_neighbor] == 3:\n",
    "            count_neighbor[i, num_neighbor+1] = 0.4\n",
    "        elif count_neighbor[i, num_neighbor] == 2:\n",
    "            count_neighbor[i, num_neighbor+1] = 0.6\n",
    "        elif count_neighbor[i, num_neighbor] == 1:\n",
    "            count_neighbor[i, num_neighbor+1] = 0.8\n",
    "        else:\n",
    "            count_neighbor[i, num_neighbor+1] = 1\n",
    "    a['degree'] = count_neighbor[:, num_neighbor+1]\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RACOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from racog import RACOG\n",
    "def rac(X_train,y_train):\n",
    "    racog = RACOG(discretization='caim', categorical_features='auto',\n",
    "                warmup_offset=100, lag0=20, n_iter='auto',\n",
    "                continous_distribution='normal', random_state=None,\n",
    "                alpha=0.6, L=0.5, threshold=5, eps=10E-5, verbose=2, n_jobs=-1)\n",
    "    x_ra,y_ra=racog.fit_resample(X_train, y_train)\n",
    "    x_ra=pd.DataFrame(x_ra,columns=X_train.columns)\n",
    "    y_ra=pd.DataFrame(y_ra)\n",
    "    return x_ra,y_ra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "def boruta_feature_selection(X, Y):  # X_test can be added here\n",
    "    \"\"\"\n",
    "    Selecting promising features with boruta algorithm\n",
    "    :param X: data features\n",
    "    :param Y: data labels\n",
    "    :return: feature vector, list of most selected features\n",
    "    \"\"\"\n",
    "    smo = BorderlineSMOTE(random_state=42,sampling_strategy='auto')\n",
    "    X, Y = smo.fit_resample(X, Y)\n",
    "    rf = RandomForestClassifier(n_jobs=-1,\n",
    "                                n_estimators=1000)\n",
    "\n",
    "    feat_selector = BorutaPy(rf,\n",
    "                             n_estimators=1000,\n",
    "                             alpha=0.05,\n",
    "                             max_iter=10, #here\n",
    "                             verbose=2)\n",
    "\n",
    "    #feat_selector.fit(X, Y)\n",
    "    feat_selector.fit(X.values, Y)\n",
    "\n",
    "    try:\n",
    "        k = 0\n",
    "        features = X.columns  # add here feature names\n",
    "        features_list = []\n",
    "        features_importance_list = []\n",
    "\n",
    "        for i in feat_selector.support_:\n",
    "            if i:\n",
    "                features_list.append(features[k])\n",
    "                features_importance_list.append(feat_selector.ranking_[k])\n",
    "            k = k + 1\n",
    "\n",
    "        features_list = [x for _, x in sorted(zip(features_importance_list, features_list))]\n",
    "    except:\n",
    "        features_list = ['no feature names given']\n",
    "\n",
    "    X_boruta = feat_selector.transform(X.values)\n",
    "\n",
    "    return features_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated Tuned RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atrf(X,Y):\n",
    "\n",
    "    # define level and initialize constructor\n",
    "    subpath = \"baseline\"\n",
    "    ct = Constants(subpath) #the pre-defination\n",
    "\n",
    "    # initialise logger\n",
    "    name_experiment = time.strftime(\"%Y%m%d_%H%M%S\") + \"_\" + str(ct.DATASETS[0])\n",
    "    logger = init_logging(ct.PATH, name_experiment)\n",
    "\n",
    "    # Loading data\n",
    "    logger.info(\"Loading data...\")\n",
    "\n",
    "\n",
    "\n",
    "    col=X.columns\n",
    "\n",
    "    var = VarianceThreshold(threshold=1.0)\n",
    "    X = var.fit_transform(X)\n",
    "    select_name_index = var.get_support(indices=True)  # 留下特征的索引值，list格式\n",
    "    select_name = []\n",
    "    for i in select_name_index:\n",
    "        select_name.append(col[i])\n",
    "    X=pd.DataFrame(X,columns=select_name)\n",
    "    \n",
    "    col=X.columns\n",
    "    X = preprocessing.scale(X)\n",
    "    X=pd.DataFrame(X,columns=col)\n",
    "\n",
    "\n",
    "    logger.info(f\"Data --> X_shape: {X.shape}, Y_shape: {Y.shape}\")\n",
    "    \n",
    "    dictionary=phcp(\n",
    "        PATH=ct.PATH,\n",
    "        name_experiment=name_experiment,\n",
    "        logger=logger,\n",
    "        subpath=subpath,\n",
    "        X=X.values,\n",
    "        Y=Y[\"label\"].values,\n",
    "    )\n",
    "    return dictionary\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    filename=\"./data_save/1_4_0.5\"\n",
    "\n",
    "    X=pd.read_csv(filename+\"_X.csv\",index_col=0)\n",
    "    y=pd.read_csv(filename+\"_y.csv\",index_col=0)\n",
    "    X=X.head(1000)\n",
    "    y=y.head(1000)\n",
    "    col=X.columns\n",
    "    var = VarianceThreshold(threshold=1.0)\n",
    "    X = var.fit_transform(X)\n",
    "    select_name_index =var.get_support(indices=True)\n",
    "    new_col=col[select_name_index]\n",
    "    X = preprocessing.scale(X)\n",
    "    X=pd.DataFrame(X,columns=new_col)\n",
    "    X_feature=boruta_feature_selection(X,y[\"label\"])\n",
    "    X=X[X_feature]\n",
    "    X,y[\"label\"]=additional_attri(X,y[\"label\"])\n",
    "    rfdic=atrf(X,y)\n",
    "    y=y[\"label\"]\n",
    "\n",
    "    kf = KFold(n_splits=10,shuffle=True)\n",
    "    f1_score=[]\n",
    "    acc=[]\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "            X_train = X.values[train_index]\n",
    "            y_train = y[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y\n",
    "            X_test = X.values[test_index]\n",
    "            y_test = y[test_index]  # See comment on ravel and  y_train\n",
    "\n",
    "            smo = BorderlineSMOTE()\n",
    "            X_train_oversampled, y_train_oversampled = smo.fit_resample(X_train, y_train)\n",
    "            model = RandomForestClassifier(n_estimators=int(rfdic[\"n_estimators\"]),max_depth=int(rfdic[\"max_depth\"]),max_features=(rfdic[\"max_depth\"]),min_samples_leaf=int(rfdic[\"min_samples_leaf\"]),min_samples_split=int(rfdic[\"min_samples_split\"]))  # Choose a model here\n",
    "            model.fit(X_train_oversampled, y_train_oversampled )  \n",
    "            y_pred = model.predict(X_test)\n",
    "            print(f'For fold {fold}:')\n",
    "            \n",
    "            print('Accuracy:', metrics.accuracy_score(y_test, y_pred))\n",
    "            print(\"F1 Score:\",metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "            acc.append(metrics.accuracy_score(y_test, y_pred))\n",
    "            f1_score.append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(\"The average Accuracy is: \",np.mean(acc))\n",
    "    print(\"The average F1 Score is: \",np.mean(f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
